## K MEANS

import numpy as np
import matplotlib.pyplot as plt

def kmeans(X, k, max_iterations=100):
    # Randomly initialize the centroids
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]

    for _ in range(max_iterations):
        # Assign each data point to the nearest centroid
        labels = assign_labels(X, centroids)

        # Update the centroids based on the mean of the assigned points
        new_centroids = update_centroids(X, labels, k)

        # Check if the centroids have converged
        if np.all(centroids == new_centroids):
            break

        centroids = new_centroids

    return labels, centroids

def assign_labels(X, centroids):
    # Calculate the Euclidean distance between each data point and centroids
    distances = np.sqrt(((X[:, np.newaxis] - centroids)**2).sum(axis=2))

    # Assign each data point to the nearest centroid
    labels = np.argmin(distances, axis=1)

    return labels

def update_centroids(X, labels, k):
    centroids = np.zeros((k, X.shape[1]))

    for i in range(k):
        # Calculate the mean of the data points assigned to the current centroid
        centroids[i] = np.mean(X[labels == i], axis=0)

    return centroids

def plot_clusters(X, labels, centroids):
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
    plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', color='red', s=200)
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.title('K-means Clustering')
    plt.show()



# Generate sample data
np.random.seed(0)
X = np.random.randn(200, 2) + np.array([[2, 2]])
X = np.concatenate([X, np.random.randn(200, 2)])

# Apply K-means clustering
k = 2
labels, centroids = kmeans(X, k)

# Visualize the clusters
plot_clusters(X, labels, centroids)
